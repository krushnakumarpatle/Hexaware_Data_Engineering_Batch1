{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70b91720-673c-4b19-aa07-aef1f1e786a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be96dd-094d-4bf4-9ecd-a60325778629",
   "metadata": {},
   "source": [
    "**GroupBy() and Aggregate Function**\n",
    "\n",
    "\r\n",
    "In PySpark, the groupBy operation is used to group data based on one or more columns and perform aggregate operations on each group. Here are some examples of how to use groupBy in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8683f1-eb45-47c4-9f7e-2dbbd1a01374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------+\n",
      "|Department|EmployeeCount|AvgSalary|\n",
      "+----------+-------------+---------+\n",
      "|     Sales|            2|   5250.0|\n",
      "|        HR|            1|   6000.0|\n",
      "|   Finance|            2|   7500.0|\n",
      "+----------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, mean\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GroupByExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = [(\"Alice\", \"Sales\", 5000),\n",
    "        (\"Bob\", \"HR\", 6000),\n",
    "        (\"Alice\", \"Finance\", 7000),\n",
    "        (\"Bob\", \"Finance\", 8000),\n",
    "        (\"Charlie\", \"Sales\", 5500)]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Group by the \"Department\" column and calculate aggregate functions\n",
    "result = df.groupBy(\"Department\") \\\n",
    "            .agg(count(\"Name\").alias(\"EmployeeCount\"), \n",
    "                 mean(\"Salary\").alias(\"AvgSalary\"))\n",
    "\n",
    "# Show the result\n",
    "result.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f72daef9-1955-4ce2-aa6a-91313311a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"PySpark_example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af81a98d-2b1b-4c1a-9b3f-5686edb69f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|   name|  department|salary|\n",
      "+-------+------------+------+\n",
      "| Chandu|Data Science| 50000|\n",
      "|Rashmi |         IOT| 75000|\n",
      "|  Rohit|    Big Data| 55000|\n",
      "| Rohit |    Big Data| 80000|\n",
      "|  Ronit|         IOT| 60000|\n",
      "|  Ronit|Data Science| 70000|\n",
      "| Chandu|Data Science| 45000|\n",
      "|Krishna|    Big Data| 65000|\n",
      "| Rashmi|    Big Data| 85000|\n",
      "|  Rohit|         IOT| 60000|\n",
      "+-------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=spark.read.csv(\"test.csv\",header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed205c-abe7-484f-8a97-f39a5e0f95c1",
   "metadata": {},
   "source": [
    "**groupBy() using multiple columns**\r",
    ")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3234919c-b908-4000-81e2-7688160becb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------+\n",
      "|   name|  department|sum(salary)|\n",
      "+-------+------------+-----------+\n",
      "|  Rohit|    Big Data|      55000|\n",
      "| Rashmi|    Big Data|      85000|\n",
      "| Chandu|Data Science|      95000|\n",
      "|  Ronit|Data Science|      70000|\n",
      "| Rohit |    Big Data|      80000|\n",
      "|  Ronit|         IOT|      60000|\n",
      "|Krishna|    Big Data|      65000|\n",
      "|Rashmi |         IOT|      75000|\n",
      "|  Rohit|         IOT|      60000|\n",
      "+-------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"name\",\"department\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654c9df-c807-43c3-a116-06799226f07f",
   "metadata": {},
   "source": [
    "**sum()**\n",
    "\r\n",
    "Compute the sum for each numeric columns for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13c3e284-b176-4914-97a3-83911e370bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     195000|\n",
      "|    Big Data|     285000|\n",
      "|Data Science|     165000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0615e391-4f58-4e92-a2ff-1d09bb580edf",
   "metadata": {},
   "source": [
    "**min()**\n",
    "\r\n",
    "Computes the min value for each numeric column for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d4eee8-55af-4b06-844a-e61b08645aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|min(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      60000|\n",
      "|    Big Data|      55000|\n",
      "|Data Science|      45000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").min(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d7af9-6f63-4a3d-adfe-2da85b13d3e8",
   "metadata": {},
   "source": [
    "**max()**\n",
    "\n",
    "Computes the max value for each numeric columns for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f59bc49-b022-450a-9565-89df493d12df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|max(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      75000|\n",
      "|    Big Data|      85000|\n",
      "|Data Science|      70000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").max(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a06d39-b9ed-49a2-8279-fb7d1136cc18",
   "metadata": {},
   "source": [
    "**avg()**\n",
    "\r\n",
    "Computes average values for each numeric columns for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f2f6dae-fb96-42dc-add0-2a3093f8b3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|    65000.0|\n",
      "|    Big Data|    71250.0|\n",
      "|Data Science|    55000.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").avg(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857262ed-ca5e-4ab8-889f-032ddc3a8094",
   "metadata": {},
   "source": [
    "**mean()**\n",
    "\r\n",
    "Computes average values for each numeric columns for each group\n",
    "\n",
    "mean() is an alias for avg().."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5cdcf18-022d-4716-9c3e-14154f96255b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|    65000.0|\n",
      "|    Big Data|    71250.0|\n",
      "|Data Science|    55000.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").mean(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddafec84-e2dc-45dd-b3ec-a6500318f2eb",
   "metadata": {},
   "source": [
    "**count()**\n",
    "\r\n",
    "Counts the number of records for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c001cc0-cbe9-4aa0-a7be-ddf03c44aa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|  department|count|\n",
      "+------------+-----+\n",
      "|         IOT|    3|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    3|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d90c6-b8ee-4f9c-8ccb-8efdca628512",
   "metadata": {},
   "source": [
    "**min()**\n",
    "\r\n",
    "Computes the min value for each numeric column for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eeebd12-0fc9-417a-9480-c4b3bcca92d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------+\n",
      "|   name|  department|min(salary)|\n",
      "+-------+------------+-----------+\n",
      "|  Rohit|    Big Data|      55000|\n",
      "| Rashmi|    Big Data|      85000|\n",
      "| Chandu|Data Science|      45000|\n",
      "|  Ronit|Data Science|      70000|\n",
      "| Rohit |    Big Data|      80000|\n",
      "|  Ronit|         IOT|      60000|\n",
      "|Krishna|    Big Data|      65000|\n",
      "|Rashmi |         IOT|      75000|\n",
      "|  Rohit|         IOT|      60000|\n",
      "+-------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"name\",\"department\").min(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931c4c8-e19c-422c-9bcb-088a4a82830f",
   "metadata": {},
   "source": [
    "**pivot(pivot_col, values=None)**\n",
    "\r\n",
    "Pivots a column of the current DataFrame and perform the specified aggregation. There are two versions of pivot function: one that requires the caller to specify the list of distinct values to pivot on, and one that does not. The latter is more concise but less efficient, because Spark needs to first compute the list of distinct values internally\n",
    "\n",
    "\r\n",
    "Parameters:\n",
    "\t \r\n",
    "pivot_col – Name of the column to piv  \n",
    " \n",
    " .\r\n",
    "values – List of values that will be translated to columns in the output DataFrame.\r\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f74a77-9234-4089-ac24-e8383ede8ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------+------+-------+-----+------+-----+\n",
      "|  department|Chandu|Krishna|Rashmi|Rashmi |Rohit|Rohit |Ronit|\n",
      "+------------+------+-------+------+-------+-----+------+-----+\n",
      "|         IOT|  NULL|   NULL|  NULL|  75000|60000|  NULL|60000|\n",
      "|    Big Data|  NULL|  65000| 85000|   NULL|55000| 80000| NULL|\n",
      "|Data Science| 95000|   NULL|  NULL|   NULL| NULL|  NULL|70000|\n",
      "+------------+------+-------+------+-------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").pivot(\"Name\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d60ae-ed56-4576-9eff-8de935681449",
   "metadata": {},
   "source": [
    "**groupBy() and agg() function**\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8305814a-71bb-4501-b274-491d8eb82549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     195000|\n",
      "|    Big Data|     285000|\n",
      "|Data Science|     165000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").agg(({\"salary\":\"sum\"})).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a341a18-e208-4dcb-a609-7630c2e9addf",
   "metadata": {},
   "source": [
    "**Handling Missing Values Pyspark**\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5909956-4d15-437c-9221-502a8a3e3bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|      Name| age|Experience|Salary|\n",
      "+----------+----+----------+------+\n",
      "|     Krish|  31|        10| 30000|\n",
      "|Shudhanshu|  30|         8| 25000|\n",
      "|     Sunny|  29|         4| 20000|\n",
      "|      Paul|  24|         3| 20000|\n",
      "|    Harsha|  21|         1| 15000|\n",
      "|   Shubham|  23|         2| 18000|\n",
      "|    Mahesh|NULL|      NULL|  NULL|\n",
      "|      NULL|NULL|        10| 40000|\n",
      "|      NULL|  34|        10| 38000|\n",
      "|      NULL|NULL|      NULL|  NULL|\n",
      "|      NULL|  36|      NULL|  NULL|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark1=spark.read.csv(\"test2.csv\",header=True,inferSchema=True)\n",
    "df_pyspark1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffcf44b-237b-40fa-b55d-1c55cbd44c21",
   "metadata": {},
   "source": [
    "**Dropping rows based on null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b89e43e-93d1-4a21-a078-ff811d405213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|age|Experience|Salary|\n",
      "+----------+---+----------+------+\n",
      "|     Krish| 31|        10| 30000|\n",
      "|Shudhanshu| 30|         8| 25000|\n",
      "|     Sunny| 29|         4| 20000|\n",
      "|      Paul| 24|         3| 20000|\n",
      "|    Harsha| 21|         1| 15000|\n",
      "|   Shubham| 23|         2| 18000|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark1.na.drop().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9823df2-1f68-49ef-a051-0e495a9df79c",
   "metadata": {},
   "source": [
    "**drop() has the following parameters — how, thresh, and subset**\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a76b321-10b2-4692-b519-0306f50cac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|      Name| age|Experience|Salary|\n",
      "+----------+----+----------+------+\n",
      "|     Krish|  31|        10| 30000|\n",
      "|Shudhanshu|  30|         8| 25000|\n",
      "|     Sunny|  29|         4| 20000|\n",
      "|      Paul|  24|         3| 20000|\n",
      "|    Harsha|  21|         1| 15000|\n",
      "|   Shubham|  23|         2| 18000|\n",
      "|    Mahesh|NULL|      NULL|  NULL|\n",
      "|      NULL|NULL|        10| 40000|\n",
      "|      NULL|  34|        10| 38000|\n",
      "|      NULL|  36|      NULL|  NULL|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark1.na.drop(how=\"all\").show()\n",
    " # if all values in rows are null then drop # default any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9edb3825-0516-4771-b0ce-2974f97fa189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|      Name| age|Experience|Salary|\n",
      "+----------+----+----------+------+\n",
      "|     Krish|  31|        10| 30000|\n",
      "|Shudhanshu|  30|         8| 25000|\n",
      "|     Sunny|  29|         4| 20000|\n",
      "|      Paul|  24|         3| 20000|\n",
      "|    Harsha|  21|         1| 15000|\n",
      "|   Shubham|  23|         2| 18000|\n",
      "|      NULL|NULL|        10| 40000|\n",
      "|      NULL|  34|        10| 38000|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark1.na.drop(how=\"any\",thresh=2).show() \n",
    "#atleast 2 non null values should be present. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "598cb0b7-b228-4403-ba16-e734843d29f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|      Name| age|Experience|Salary|\n",
      "+----------+----+----------+------+\n",
      "|     Krish|  31|        10| 30000|\n",
      "|Shudhanshu|  30|         8| 25000|\n",
      "|     Sunny|  29|         4| 20000|\n",
      "|      Paul|  24|         3| 20000|\n",
      "|    Harsha|  21|         1| 15000|\n",
      "|   Shubham|  23|         2| 18000|\n",
      "|      NULL|NULL|        10| 40000|\n",
      "|      NULL|  34|        10| 38000|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark1.na.drop(how=\"any\",subset=[\"salary\"]).show()\n",
    " # only in that column rows get deleted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62461198-8818-4e77-9eeb-0b32c5cb56cc",
   "metadata": {},
   "source": [
    "**Filling missing values — Single Value**\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28c18444-3007-4f49-bbe2-d7a72b61ceb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|         Krish|  31|        10| 30000|\n",
      "|    Shudhanshu|  30|         8| 25000|\n",
      "|         Sunny|  29|         4| 20000|\n",
      "|          Paul|  24|         3| 20000|\n",
      "|        Harsha|  21|         1| 15000|\n",
      "|       Shubham|  23|         2| 18000|\n",
      "|        Mahesh|NULL|      NULL|  NULL|\n",
      "|Missing Values|NULL|        10| 40000|\n",
      "|Missing Values|  34|        10| 38000|\n",
      "|Missing Values|NULL|      NULL|  NULL|\n",
      "|Missing Values|  36|      NULL|  NULL|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark1.na.fill('Missing Values').show() #string values will get replaced as string is given as input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26f4af69-c1f8-457d-950d-24571878c009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|age|Experience|Salary|\n",
      "+----------+---+----------+------+\n",
      "|     Krish| 31|        10| 30000|\n",
      "|Shudhanshu| 30|         8| 25000|\n",
      "|     Sunny| 29|         4| 20000|\n",
      "|      Paul| 24|         3| 20000|\n",
      "|    Harsha| 21|         1| 15000|\n",
      "|   Shubham| 23|         2| 18000|\n",
      "|    Mahesh|  0|         0|     0|\n",
      "|      NULL|  0|        10| 40000|\n",
      "|      NULL| 34|        10| 38000|\n",
      "|      NULL|  0|         0|     0|\n",
      "|      NULL| 36|         0|     0|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark1.na.fill(0).show() \n",
    "#integer values will get replaced as integer is given as input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac615d-721c-4147-a57a-39e9a0f7823b",
   "metadata": {},
   "source": [
    "**Filling missing values using Mean, Median, or Mode with help of the Imputer function**\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f5fb6a8-7938-4811-8d64-f352493717d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling with mean \n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(inputCols=[\"age\"],outputCols=[\"age_imputed\"]).setStrategy(\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f40f99c-c765-4ac9-9926-c350293ed46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+-----------+\n",
      "|      Name| age|Experience|Salary|age_imputed|\n",
      "+----------+----+----------+------+-----------+\n",
      "|     Krish|  31|        10| 30000|         31|\n",
      "|Shudhanshu|  30|         8| 25000|         30|\n",
      "|     Sunny|  29|         4| 20000|         29|\n",
      "|      Paul|  24|         3| 20000|         24|\n",
      "|    Harsha|  21|         1| 15000|         21|\n",
      "|   Shubham|  23|         2| 18000|         23|\n",
      "|    Mahesh|NULL|      NULL|  NULL|         28|\n",
      "|      NULL|NULL|        10| 40000|         28|\n",
      "|      NULL|  34|        10| 38000|         34|\n",
      "|      NULL|NULL|      NULL|  NULL|         28|\n",
      "|      NULL|  36|      NULL|  NULL|         36|\n",
      "+----------+----+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark1).transform(df_pyspark1).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813849aa-7249-43c6-8b84-c15f66e79189",
   "metadata": {},
   "source": [
    "**orderBy() and sort() in Pyspark DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71b3a576-681a-4a02-bcbc-a241c99a7776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|   name|  department|salary|\n",
      "+-------+------------+------+\n",
      "| Chandu|Data Science| 50000|\n",
      "|Rashmi |         IOT| 75000|\n",
      "|  Rohit|    Big Data| 55000|\n",
      "| Rohit |    Big Data| 80000|\n",
      "|  Ronit|         IOT| 60000|\n",
      "|  Ronit|Data Science| 70000|\n",
      "| Chandu|Data Science| 45000|\n",
      "|Krishna|    Big Data| 65000|\n",
      "| Rashmi|    Big Data| 85000|\n",
      "|  Rohit|         IOT| 60000|\n",
      "+-------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa03959-fe04-486a-a429-ebb677f36bb9",
   "metadata": {},
   "source": [
    "**sort() — To sort a dataframe by using one or more columns, Default — ascending order**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b36e853-5300-4d2c-824a-4bfd7ba4e583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|   name|  department|salary|\n",
      "+-------+------------+------+\n",
      "| Chandu|Data Science| 45000|\n",
      "| Chandu|Data Science| 50000|\n",
      "|  Rohit|    Big Data| 55000|\n",
      "|  Ronit|         IOT| 60000|\n",
      "|  Rohit|         IOT| 60000|\n",
      "|Krishna|    Big Data| 65000|\n",
      "|  Ronit|Data Science| 70000|\n",
      "|Rashmi |         IOT| 75000|\n",
      "| Rohit |    Big Data| 80000|\n",
      "| Rashmi|    Big Data| 85000|\n",
      "+-------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.sort(\"salary\").show() # Sort based on single column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ad44c41-1d3e-4c4d-ba79-7cfc55f6f0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|   name|  department|salary|\n",
      "+-------+------------+------+\n",
      "| Rashmi|    Big Data| 85000|\n",
      "| Rohit |    Big Data| 80000|\n",
      "|Rashmi |         IOT| 75000|\n",
      "|  Ronit|Data Science| 70000|\n",
      "|Krishna|    Big Data| 65000|\n",
      "|  Ronit|         IOT| 60000|\n",
      "|  Rohit|         IOT| 60000|\n",
      "|  Rohit|    Big Data| 55000|\n",
      "| Chandu|Data Science| 50000|\n",
      "| Chandu|Data Science| 45000|\n",
      "+-------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.sort(df_pyspark[\"salary\"].desc()).show() # sort based on descending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07b92c8d-dff3-4ab8-924a-d471d7285b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|   name|  department|salary|\n",
      "+-------+------------+------+\n",
      "| Chandu|Data Science| 45000|\n",
      "| Chandu|Data Science| 50000|\n",
      "|  Rohit|    Big Data| 55000|\n",
      "|  Rohit|         IOT| 60000|\n",
      "|  Ronit|         IOT| 60000|\n",
      "|Krishna|    Big Data| 65000|\n",
      "|  Ronit|Data Science| 70000|\n",
      "|Rashmi |         IOT| 75000|\n",
      "| Rohit |    Big Data| 80000|\n",
      "| Rashmi|    Big Data| 85000|\n",
      "+-------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.sort(\"salary\",\"name\").show() # Sort based on first column then second column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a24d55a-673f-4f40-ab41-7130799da027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|   name|  department|salary|\n",
      "+-------+------------+------+\n",
      "| Chandu|Data Science| 45000|\n",
      "| Chandu|Data Science| 50000|\n",
      "|  Rohit|    Big Data| 55000|\n",
      "|  Ronit|         IOT| 60000|\n",
      "|  Rohit|         IOT| 60000|\n",
      "|Krishna|    Big Data| 65000|\n",
      "|  Ronit|Data Science| 70000|\n",
      "|Rashmi |         IOT| 75000|\n",
      "| Rohit |    Big Data| 80000|\n",
      "| Rashmi|    Big Data| 85000|\n",
      "+-------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.orderBy(\"salary\").show() # Sort based on single column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12bd3a-fae9-4e31-95e7-77db018d4105",
   "metadata": {},
   "source": [
    "**join() using pyspark** :\n",
    "\r\n",
    "PySpark Join is used to combine two DataFrames and by chaining these you can join multiple DataFrames;\n",
    "\n",
    " it supports all basic join type operations available in traditional SQL like INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF join.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de6caa02-e386-41d9-8c1a-5a2ac27d0f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000),(2, \"Rose\",1 , \"2010\", \"20\",\"M\", 4000),(3,\"Williams\",1,\"2010\",\"10\",\"M\",1000),(4, \"Jones\",2 ,\"2005\",\"10\",\"F\",2000),(5,\"Brown\",2,\"2010\",\"40\",\"\",-1),(6, \"Brown\", 2, \"2010\",\"50\",\"\",-1)]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06362ec1-f6a1-4398-9f54-92059b811bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0351d7-76be-4473-bdae-a7eab260b188",
   "metadata": {},
   "source": [
    "**Inner Join**\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "132d2d9d-ba88-44c2-b916-16a133555a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc7b43c-7d5b-48a3-8e93-0271e8f02c80",
   "metadata": {},
   "source": [
    "**Outer Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "474d44be-f310-4d1b-8b43-ee6b98a6ba1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|  NULL|    NULL|           NULL|       NULL|       NULL|  NULL|  NULL|    Sales|     30|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|     NULL|   NULL|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c6d08-f860-484c-b689-55b3a075308b",
   "metadata": {},
   "source": [
    "**Left Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efdb90a2-54db-40cc-9d8a-9130a1c69e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|     NULL|   NULL|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a813883a-c016-4e1f-aab8-729829f164c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|     NULL|   NULL|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9861faa3-ba62-4942-85d7-76198b0e4205",
   "metadata": {},
   "source": [
    "**Right Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e85574b4-4c15-43d2-83e8-1b96ad7483e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|  NULL|    NULL|           NULL|       NULL|       NULL|  NULL|  NULL|    Sales|     30|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08225885-e51b-4bc8-bb2d-b21dd38f3d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|  NULL|    NULL|           NULL|       NULL|       NULL|  NULL|  NULL|    Sales|     30|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fa8287-e769-4b37-986a-2169d49ded47",
   "metadata": {},
   "source": [
    "**Leftsemi Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1519b9f6-152e-4f76-943e-aafa89fe5e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0bc1c9-9158-4acd-bc6b-72a94ad064d4",
   "metadata": {},
   "source": [
    "**Left Anti Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49d4b1a9-894d-481e-9ea4-5cf9fc03f212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|     6|Brown|              2|       2010|         50|      |    -1|\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b0697-b52b-489d-bae6-9f87b9778899",
   "metadata": {},
   "source": [
    "**Union() using pyspark**\r\n",
    "\n",
    "To merge two or more dataframes of same schema or structure union() is used.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29105de6-6aaf-45fc-a4ff-c31a0c0dc40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|Student Name|Overall Percentage|\n",
      "+------------+------------------+\n",
      "|   Bhuwanesh|             82.98|\n",
      "|     Harshit|             80.31|\n",
      "|      Naveen|            91.123|\n",
      "|      Piyush|             90.51|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe \n",
    "data_frame1 = spark.createDataFrame( \n",
    "    [(\"Bhuwanesh\", 82.98), (\"Harshit\", 80.31)], \n",
    "    [\"Student Name\", \"Overall Percentage\"] \n",
    ") \n",
    "  \n",
    "# Creating another dataframe \n",
    "data_frame2 = spark.createDataFrame( \n",
    "    [(\"Naveen\", 91.123), (\"Piyush\", 90.51)], \n",
    "    [\"Student Name\", \"Overall Percentage\"] \n",
    ") \n",
    "  \n",
    "# union() \n",
    "answer = data_frame1.union(data_frame2) \n",
    "  \n",
    "# Print the result of the union() \n",
    "answer.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36386f67-b248-4868-92fd-844ff6ada551",
   "metadata": {},
   "source": [
    "**UnionAll()  in PySpark**\r\n",
    "\n",
    "UnionAll() function does the same task as union() function but this function is deprecated since Spark “2.0.0” version. Hence, union() function is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "637c41b9-84b5-491d-8cc2-310dd1bea99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|Student Name|Overall Percentage|\n",
      "+------------+------------------+\n",
      "|   Bhuwanesh|             82.98|\n",
      "|     Harshit|             80.31|\n",
      "|      Naveen|            91.123|\n",
      "|      Piyush|             90.51|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = data_frame1.unionAll(data_frame2) \n",
    "  \n",
    "# Print the union of both the dataframes \n",
    "answer.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d0c5ba-6e58-4478-9cae-590d1ec60c0e",
   "metadata": {},
   "source": [
    "**Merge without Duplicates**\n",
    "\r\n",
    "Since the union() method returns all rows without distinct records, we will use\r\n",
    "t h**e distinct**() function to return just one record when a duplicate exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f465768-85eb-4f17-b7c1-75dadc5dca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "(\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "(\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "(\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
    "]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "(\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "(\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "(\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "(\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "]\n",
    "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "unionDF = df.union(df2)\n",
    "unionDF.show(truncate=False)\n",
    "disDF = df.union(df2).distinct()\n",
    "disDF.show(truncate=False)\n",
    "unionAllDF = df.unionAll(df2)\n",
    "unionAllDF.show(truncate=False)\n",
    "unionAllDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c141087-b8bc-4fbb-ad46-09bd119f6bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
